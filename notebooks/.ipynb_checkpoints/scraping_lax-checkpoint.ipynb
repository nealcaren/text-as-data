{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Web scraping in Python\n",
      "---------------------\n",
      "I find sports more exciting when I know what to expect. How many points\n",
      "is our opponent scoring relative to how they did in the past? Is our offense\n",
      "shooting lights out or are they running up the score against a struggling defense? For many sports,\n",
      "Las Vegas sports books provides me the information I want with the line (expected difference \n",
      "between the home team's score and the visitor's score) and the total (the sum of the \n",
      "expected scores). For example, at game time on Monday, March 11th,\n",
      "Iona's men's basketball team was a four-point favorite (-4) over Manhattan, with a total\n",
      "of 117. While there may be some biases in these numbers, they are fairly accurate guides of\n",
      "what to expect.\n",
      "\n",
      "At the moment, I'm interested in women's college lacrosse. Unfortunately, Vegas doesn't provide\n",
      "lines for lacrosse (or any other college sport\n",
      "but football and men's basketball. [Laxpower](http://www.laxpower.com/update13/binwom/rating01.php)\n",
      "has a power rating, which a numerical measure of the strength of a team. Comparing two team's ratings and adding about a home\n",
      "field advantage does give a pretty good line,\n",
      "but they don't provide total projections. Since expected point differences and totals for\n",
      "lacrosse aren't\n",
      "available anywhere else, I decided to create my own lines and totals for entertainment\n",
      "and educational purposes. You can see the [results](http://www.unc.edu/~ncaren/lax.html).\n",
      "\n",
      "Creating lines involves: finding and downloading game data; developing\n",
      "a power rating model to rank each team; and using those models to predict \n",
      "future games. This general process isn't limited to constructing sports power rankings; \n",
      "these are the same general steps used to scrape\n",
      "data from websites for any sort of quantitative analysis. It's the same process \n",
      "I and my coauthors used, for example, to (analyze)[http://nealcaren.web.unc.edu/files/2012/05/smoc.pdf]\n",
      "a white racist web forum.\n",
      "\n",
      "Luckily, Laxpower has all the game information, both for games played and scheduled.\n",
      "I wanted to cycle through each of the pages to get the information, but first\n",
      "I needed to know the URLs for all those pages. Luckily the [ranking page](http://www.laxpower.com/update13/binwom/rating01.php)\n",
      "has all the teams listed along with links to their pages, so I can grab the information\n",
      "from there. This two-step processes-finding the URLs you want to download and then visiting each of them-is\n",
      "fairly common for this kind of web scraping research.\n",
      "\n",
      "I start by visiting the page I want to scrape in my browser and finding an example of the type\n",
      "of information I want to get, such as the text `North Carolina` which I know, based on clicking it,\n",
      "leads to all the game information for the North Carolina team. Again, in my browser, I viewed\n",
      "the source for the ranking page, that is the raw HTML. In this view, I searched\n",
      "for \"North Carolina\" again so I could get a sense of what each link looked like in the code.  \n",
      "Fortunately, \n",
      "the page had two pieces of information for each team listed in a way that was very easy\n",
      "to extract. Each link began with a `\"` and ended in `PHP\"`. Acutally, this is just\n",
      "the relative path to the URL. I'll fill in the beginning part of the URL later). This was followed by a `>`,\n",
      "the school's name, and then a `>'. This is a situation where a simple\n",
      "[regular expression](http://www.regular-expressions.info) would allow me to pull\n",
      "out the information I needed.\n",
      "\n",
      "To get my list that contains all the URLs, I could take advantage of uniform\n",
      "way they were listed. In the Python variant of regular expressions the powerful \n",
      "combination of `.*?` will find any character, repeated any number of times, until it\n",
      "runs into something else. So searching a text for `My .*? dog` would grab all the text \n",
      "between `My` and `dog`. In my case, I wanted to extract all the instances\n",
      "of text that occurred between a quotation mark and `PHP` followed by a quotation mark, so\n",
      "I could search for instances of `\".*?PHP\"` in the page's text. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "import re\n",
      "\n",
      "teams_html=urllib2.urlopen('http://www.laxpower.com/update13/binwom/rating01.php').read()\n",
      "teams=re.findall('\".*?PHP\"',teams_html)\n",
      "print teams[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['\"XMADXX.PHP\"', '\"XUFLXX.PHP\"', '\"XNWSXX.PHP\"', '\"XUNCXX.PHP\"', '\"XSYRXX.PHP\"']"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is pretty good, but I don't want the quotation marks. I can be pickier \n",
      "about what I extract by using parentheses, which instructs `re` to only\n",
      "return the stuff between parentheses."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "teams=re.findall('\"(.*?PHP)\"',teams_html)\n",
      "print teams[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['XMADXX.PHP', 'XUFLXX.PHP', 'XNWSXX.PHP', 'XUNCXX.PHP', 'XSYRXX.PHP']"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As I noted above, next to this is also the school's name. I can\n",
      "extract this as well by extending the `re` statement."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "teams=re.findall('\"(.*?PHP)\">(.*?)<',teams_html)\n",
      "print teams[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('XMADXX.PHP', 'Maryland'), ('XUFLXX.PHP', 'Florida'), ('XNWSXX.PHP', 'Northwestern'), ('XUNCXX.PHP', 'North Carolina'), ('XSYRXX.PHP', 'Syracuse')]"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Adding `>(.*?)<` had the effect of extending the search and returning everything between\n",
      "the greater than and less than signs. This is returned as a list of tuples. Note that \n",
      "regular expressions are complicated and more times than not will return either nothing\n",
      "or the entire text of the document. Trial, error, and reading are the only way forward.\n",
      "\n",
      "I want to remove any duplicates by turning the returned list into \n",
      "a set, and then back into a list."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(teams)\n",
      "teams=list(set(teams))\n",
      "print len(teams)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "200\n",
        "100"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I also want to store it in more useful format. I might forget later on whether the team\n",
      "or the URL was first in the tuple. Can't go wrong with a list of dictionaries. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "teams=[{'team id':t[0],'team name':t[1]} for t in teams]\n",
      "print teams[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[{'team name': 'Lehigh', 'team id': 'XLEHXX.PHP'}, {'team name': 'Columbia', 'team id': 'XCMBXX.PHP'}, {'team name': 'Boston University', 'team id': 'XBOUXX.PHP'}, {'team name': 'Princeton', 'team id': 'XPRIXX.PHP'}, {'team name': 'Quinnipiac', 'team id': 'XQUIXX.PHP'}]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that I know all the teams and where to get information about them, I want to go \n",
      "to each of those pages and get the information about each game-who,when,where, and if\n",
      "it has already been played, what the score was. A quick look at the source\n",
      "for a [page](http://www.laxpower.com/update13/binwom/XMADXX.PHP) shows that information\n",
      "is stored in an HTML table. This is good news since other ways of presenting data \n",
      "on the page can be hard to get, while other ways the information shows up, such as those displayed using Flash,\n",
      "can be impossible.\n",
      "\n",
      "I'm going to use the [`BeautifulSoup`](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) module to help parse the HTML. Regular expressions\n",
      "can get you pretty far, but modules like `BeautifulSoup` can save you a lot of time. They\n",
      "are much easier to use if you already know things like what a DOM element is, but are still\n",
      "usable for those who don't code web pages. \n",
      "\n",
      "After downloading, opening, and soupifying the page (see the function below), you can extract the table\n",
      "with a simple `table = soup.find(\"table\")` while `rows=table.find_all(\"tr\")` will identify each of the\n",
      "rows.  You might not want the first or last rows, depending on how the information\n",
      "is presented, so you can slice by appending something like `[-1:]` which will\n",
      "start after the first row. \n",
      "\n",
      "Within in each row, you can extract a list of the cells with \n",
      "`cell=row.findAll('td')`. Another powerful feature of `BeautifulSoup` is that you can\n",
      "get rid of the HTML formatting with `.get_text()' which is a lot more efficient \n",
      "than a complicated regular expression, which might not always work. In my case, I'm\n",
      "not going to sort through the contents of each of the cells here. Since I want all the\n",
      "information, I'm just going to dump it to a file and organize it later.\n",
      "\n",
      "My `get_team_page' function downloads the page and then extracts the contents of\n",
      "all the informative rows of the table and returns them as a list of lists. In \n",
      "retrospect, this should probably be split into two functions, \n",
      "with one that downloads the page and another that extracts the table information. \n",
      " That second function would be useful in other contexts, so I could use it in\n",
      "other projects. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "\n",
      "def get_team_page(team):\n",
      "    team_url='http://www.laxpower.com/update13/binwom/%s?tab=detail' % team['team id']\n",
      "    team_html=urllib2.urlopen(team_url).read()\n",
      "    soup = BeautifulSoup(team_html.decode('utf-8', 'ignore'))\n",
      "    table = soup.find(\"table\")\n",
      "    rows=[]\n",
      "    for row in table.find_all(\"tr\")[3:-1]:\n",
      "        data=[d.get_text().replace(u'\\xa0\\x96','') for d in row.findAll('td')]\n",
      "        outline=[team['team name']]+data[:5]\n",
      "        rows.append([i.encode('utf-8') for i in outline])\n",
      "    return rows "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For maximum flexiblity, I want to output all the data to a tab separated file. I\n",
      "do this with the `csv' module. The `\\t` tells the writer to use a tab instead\n",
      "of the default comma between items."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import csv\n",
      "outfile=csv.writer(open('lax_13.tsv','wb'),delimiter='\\t')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to be polite to the website, I want to pause a second between\n",
      "each page. Usually I try to save the contents of each page locally\n",
      "so that I only have to download it once. In this case, I'll be running it\n",
      "everyday and I want the most recent results, so I'm not going to save each\n",
      "page. One problem with this script is that the function above \n",
      "will crash if the web server is down\n",
      "or any other sort of HTML error. A better function would put the `urllib2.urlopen()` \n",
      "in a `try:` so that it can skip over those pages, if you think that is acceptable. \n",
      "Otherwise, you might have it so that if it can't download the page, it loads up\n",
      "the most recent locally saved version. All depends on what the data is and\n",
      "what you want to do with it.\n",
      "\n",
      "The loop that goes through each team, downloads the page,\n",
      "returns the table, and then writes the results to a file has a `print` line so that \n",
      "I can watch it go and see if its getting hung up anywhere. I've commented it out here because\n",
      "it made this page too long."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from time import sleep\n",
      "\n",
      "for team in teams:\n",
      "    #print team['team name']\n",
      "    game_history = get_team_page(team)\n",
      "    outfile.writerows(game_history)\n",
      "    #pasue to be polite to the website\n",
      "    sleep(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The resulting file, `lax_13.tsv`, can be read in any statistical program or in Excel\n",
      "if you want to do your analysis there. To check that it worked, we can open it up\n",
      "and print out the first few rows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "game_info=csv.reader(open('lax_13.tsv','rb'), delimiter='\\t')\n",
      "\n",
      "for row in list(game_info)[:5]:\n",
      "    print row"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['Lehigh', '02/16', 'H', 'Villanova', '10', '9']\n",
        "['Lehigh', '02/23', 'A', 'Temple', '7', '14']\n",
        "['Lehigh', '02/27', 'A', 'Binghamton', '11', '10']\n",
        "['Lehigh', '03/05', 'H', 'Delaware', '7', '17']\n",
        "['Lehigh', '03/09', 'A', 'Navy', '4', '14']"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looks good to me."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#ignore code below. Imports style sheet for this page.\n",
      "\n",
      "from IPython.core.display import HTML\n",
      "def css_styling():\n",
      "    styles = open(\"custom.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<style>\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
        "    }\n",
        "    div.cell{\n",
        "        width:800px;\n",
        "        margin-left:auto;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "    h1 {\n",
        "        font-family: \"Computer Modern\";\n",
        "        \n",
        "    }\n",
        "    h4{\n",
        "        margin-top:12px;\n",
        "        margin-bottom: 3px;\n",
        "        \n",
        "       }\n",
        "    div.text_cell_render{\n",
        "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
        "        line-height: 145%;\n",
        "        font-size: 130%;\n",
        "        width:800px;\n",
        "        margin-left:auto;\n",
        "        margin-right:auto;\n",
        "                color: #413839;\n",
        "\n",
        "    }\n",
        "    .CodeMirror{\n",
        "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
        "            \n",
        "    }\n",
        "    .prompt{\n",
        "        display: None;\n",
        "    }\n",
        "    .text_cell_render h5 {\n",
        "        font-weight: 300;\n",
        "        font-size: 16pt;\n",
        "        color: #4057A1;\n",
        "        font-style: italic;\n",
        "        margin-bottom: .5em;\n",
        "        margin-top: 0.5em;\n",
        "        display: block;\n",
        "    }\n",
        "\n",
        "    .warning{\n",
        "        color: rgb( 240, 20, 20 )\n",
        "        }\n",
        "\n",
        "</style>\n"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "<IPython.core.display.HTML at 0x103e39f10>"
       ]
      }
     ],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}